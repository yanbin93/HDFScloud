{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以iris数据集（iris）为例进行分析。iris以鸢尾花的特征作为数据来源，数据集包含150个数据集，分为3类，每类50个数据，每个数据包含4个属性，是在数据挖掘、数据分类中非常常用的测试集、训练集。为了便于理解，我们这里主要用后两个属性（花瓣的长度和宽度）来进行分类。目前 spark.ml 中支持二分类和多分类，我们将分别从“用二项逻辑斯蒂回归来解决二分类问题”、“用多项逻辑斯蒂回归来解决二分类问题”、“用多项逻辑斯蒂回归来解决多分类问题”三个方面进行分析\n",
    "## 二项逻辑斯蒂回归解决 二分类 问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5.1,3.5,1.4,0.2,Iris-setosa\r\n",
      "4.9,3.0,1.4,0.2,Iris-setosa\r\n",
      "4.7,3.2,1.3,0.2,Iris-setosa\r\n",
      "4.6,3.1,1.5,0.2,Iris-setosa\r\n",
      "5.0,3.6,1.4,0.2,Iris-setosa\r\n"
     ]
    }
   ],
   "source": [
    "!hadoop fs -cat /user/yanbin/data/iris.txt|head -n5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkConf,SparkContext\n",
    "from pyspark.ml import Pipeline,PipelineModel\n",
    "from pyspark.ml.linalg import Vector,Vectors\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "from pyspark.ml.classification import LogisticRegression,LogisticRegressionModel\n",
    "from pyspark.ml.classification import BinaryLogisticRegressionSummary\n",
    "from pyspark.ml.feature import StringIndexer,IndexToString,HashingTF,Tokenizer,VectorIndexer\n",
    "from pyspark.sql import Row\n",
    "conf = SparkConf().setAppName('logist Regession')\\\n",
    "                  .setMaster('local')\n",
    "spark = SparkSession.builder\\\n",
    "        .config(conf=conf)\\\n",
    "        .enableHiveSupport()\\\n",
    "        .getOrCreate()\n",
    "sc = spark.sparkContext\n",
    "\n",
    "path = 'data/iris.txt'\n",
    "\n",
    "rawdata = sc.textFile(path).map(lambda line:line.split(\",\"))\\\n",
    "          .map(lambda x:Row(Vectors.dense(float(x[0]),float(x[1]),float(x[2]),float(x[3])),x[4]))\n",
    "rawDf = spark.createDataFrame(rawdata,[\"features\",\"label\"])\n",
    "df=rawDf.filter(\"label!='Iris-setosa'\")\n",
    "\n",
    "labelInder = StringIndexer(inputCol='label',outputCol='indexedLabel').fit(df)\n",
    "featureIndex = VectorIndexer(inputCol='features',outputCol='indexedFeatures').fit(df)\n",
    "(traningDf,testDf) = df.randomSplit([0.7,0.3])\n",
    "lr = LogisticRegression(featuresCol='indexedFeatures',labelCol='indexedLabel',\\\n",
    "                        maxIter=10,regParam=0.3,elasticNetParam=0.8)\n",
    "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel',labels=labelInder.labels)\n",
    "lrPipeline = Pipeline().setStages([labelInder,featureIndex,lr,labelConverter])\n",
    "lrPipelineModel = lrPipeline.fit(traningDf)\n",
    "lrPredictions = lrPipelineModel.transform(testDf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " (Iris-virginica, Iris-versicolor --> prob=[5.4,3.0,4.5,1.5], predicted Label=[0.472370451576,0.527629548424]\n",
      "(Iris-versicolor, Iris-versicolor --> prob=[5.5,2.4,3.7,1.0], predicted Label=[0.56719423018,0.43280576982]\n",
      "(Iris-versicolor, Iris-versicolor --> prob=[5.5,2.6,4.4,1.2], predicted Label=[0.529932895954,0.470067104046]\n",
      "(Iris-virginica, Iris-versicolor --> prob=[5.6,3.0,4.5,1.5], predicted Label=[0.474713416283,0.525286583717]\n",
      "(Iris-virginica, Iris-virginica --> prob=[5.7,2.5,5.0,2.0], predicted Label=[0.383935252367,0.616064747633]\n",
      "('Test Error = ', 0.2054298642533936)\n",
      "Coefficients:  [[-0.04699096  0.          0.          0.07526891]] \n",
      "Intercept:  [-0.0119624567944] \n",
      "numClasses:  2 \n",
      "numFeatures:  4\n",
      "[0.6927389617440812, 0.6899568172038798, 0.6884621802365235, 0.6871411658539227, 0.683999190620436, 0.6742600596548816, 0.6729880325825707, 0.6729720010493697, 0.6719400739860806, 0.6717587146812963, 0.6712425059750556]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'setCallSite'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-219-7a5ab3204c69>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m \u001b[0mpD\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrainingSummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mroc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtoPandas\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;31m# plt.plot(pD.ix[:,0],pD.ix[:,1])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0mpD_FPR\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpD\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'FPR'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/hadoop-2.6/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mtoPandas\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1574\u001b[0m         \"\"\"\n\u001b[1;32m   1575\u001b[0m         \u001b[0;32mimport\u001b[0m \u001b[0mpandas\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1576\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_records\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollect\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1578\u001b[0m     \u001b[0;31m##########################################################################################\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/hadoop-2.6/spark/python/pyspark/sql/dataframe.pyc\u001b[0m in \u001b[0;36mcollect\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    388\u001b[0m         \u001b[0;34m[\u001b[0m\u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Alice'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mage\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34mu'Bob'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m         \"\"\"\n\u001b[0;32m--> 390\u001b[0;31m         \u001b[0;32mwith\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sc\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mcss\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    391\u001b[0m             \u001b[0mport\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollectToPython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_load_from_socket\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mport\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mBatchedSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPickleSerializer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/home/yanbin/hadoop-2.6/spark/python/pyspark/traceback_utils.pyc\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     70\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__enter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 72\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jsc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetCallSite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_site\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     73\u001b[0m         \u001b[0mSCCallSiteSync\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_spark_stack_depth\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'setCallSite'"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "result = lrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").rdd\\\n",
    ".map(lambda x:(\"(%s, %s --> prob=%s, predicted Label=%s\") %(x.predictedLabel,x.label,x.features,x.probability))\\\n",
    ".take(5)\n",
    "for i in result:\n",
    "    print i\n",
    "evaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel',predictionCol='prediction')\n",
    "lrAccuracy = evaluator.evaluate(lrPredictions)\n",
    "print ('Test Error = ',1-lrAccuracy)\n",
    "lrModel = lrPipelineModel.stages[2]\n",
    "print \"Coefficients: \" , lrModel.coefficientMatrix.toArray(),\"\\n\"\\\n",
    "        \"Intercept: \",lrModel.interceptVector,\"\\n\"\\\n",
    "        \"numClasses: \",lrModel.numClasses,\"\\n\"\\\n",
    "        \"numFeatures: \",lrModel.numFeatures\n",
    "trainingSummary = lrModel.summary\n",
    "objectiveHistory = trainingSummary.objectiveHistory\n",
    "print objectiveHistory\n",
    "# Obtain the receiver-operating characteristic as a dataframe and areaUnderROC.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "pD=trainingSummary.roc.toPandas()\n",
    "# plt.plot(pD.ix[:,0],pD.ix[:,1])\n",
    "pD_FPR=pD.set_index('FPR')\n",
    "pD_FPR.plot()\n",
    "plt.legend(loc='best')\n",
    "FDf=trainingSummary.fMeasureByThreshold.sort('threshold')\n",
    "maxFMeasure =FDf.select(max('F-Measure').alias('maxFMeasure')).head().maxFMeasure\n",
    "print 'maxFMeasure: ',maxFMeasure\n",
    "FDf=FDf.withColumnRenamed('F-Measure','F1')\n",
    "# FDf.select(FDf.F1.cast(FloatType())).show()\n",
    "# FDf.createTempView('F1Table2')\n",
    "bestThreshold = FDf.where((-0.00001<(FDf.F1-maxFMeasure)) & ((FDf.F1-maxFMeasure)<0.0001)).head()['threshold']\n",
    "print 'bestThreshold: ',bestThreshold\n",
    "lr.setThreshold(bestThreshold)\n",
    "# pDf=trainingSummary.precisionByThreshold\n",
    "# rDf=trainingSummary.recallByThreshold\n",
    "# rDf=rDf.withColumnRenamed('threshold','threshold1')\n",
    "# f=rDf.join(pDf,pDf.threshold==rDf.threshold1,'left')\n",
    "# f.select(f.threshold,(2/((1.0/f.recall)+(1.0/f.precision))).alias('F1')).sort('threshold').show()\n",
    "print(\"areaUnderROC: \" + str(trainingSummary.areaUnderROC))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用多项逻辑斯蒂回归解决 二分类 问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iris-virginica, Iris-versicolor --> prob=[5.4,3.0,4.5,1.5], predicted Label=[0.468153771927,0.531846228073]\n",
      "(Iris-versicolor, Iris-versicolor --> prob=[5.5,2.4,3.7,1.0], predicted Label=[0.578495995776,0.421504004224]\n",
      "(Iris-versicolor, Iris-versicolor --> prob=[5.5,2.6,4.4,1.2], predicted Label=[0.535266692919,0.464733307081]\n",
      "(Iris-virginica, Iris-versicolor --> prob=[5.6,3.0,4.5,1.5], predicted Label=[0.471096653482,0.528903346518]\n",
      "(Iris-virginica, Iris-virginica --> prob=[5.7,2.5,5.0,2.0], predicted Label=[0.36630830832,0.63369169168]\n",
      "Coefficients:  [[ 0.02953791  0.          0.         -0.04382524]\n",
      " [-0.02953791  0.          0.          0.04382524]] \n",
      "Intercept:  [-0.00415734261051,0.00415734261051] \n",
      "numClasses:  2 \n",
      "numFeatures:  4\n",
      "('Test Error = ', 0.2054298642533936)\n"
     ]
    }
   ],
   "source": [
    "mlr = LogisticRegression(featuresCol='indexedFeatures',labelCol='indexedLabel',\\\n",
    "                         maxIter=10,regParam=0.3,elasticNetParam=0.8,family='multinomial')\n",
    "mlrPipeline = Pipeline().setStages([labelInder,featureIndex,mlr,labelConverter])\n",
    "mlrPipelineModel = mlrPipeline.fit(traningDf)\n",
    "mlrPredictions = mlrPipelineModel.transform(testDf)\n",
    "result2 =mlrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").rdd\\\n",
    ".map(lambda x:(\"(%s, %s --> prob=%s, predicted Label=%s\") %(x.predictedLabel,x.label,x.features,x.probability)).take(5)\n",
    "for i in result2:\n",
    "    print i\n",
    "mEvaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel',predictionCol='prediction')\n",
    "mlrAccuracy = evaluator.evaluate(mlrPredictions)\n",
    "mlrModel = mlrPipelineModel.stages[2]\n",
    "print \"Coefficients: \" , mlrModel.coefficientMatrix.toArray(),\"\\n\"\\\n",
    "        \"Intercept: \",mlrModel.interceptVector,\"\\n\"\\\n",
    "        \"numClasses: \",mlrModel.numClasses,\"\\n\"\\\n",
    "        \"numFeatures: \",mlrModel.numFeatures\n",
    "mlrAccuracy = mEvaluator.evaluate(mlrPredictions)\n",
    "print ('Test Error = ',1-mlrAccuracy)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 用多项逻辑斯蒂回归解决 多分类 问题"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Iris-setosa, Iris-setosa --> prob=[4.3,3.0,1.1,0.1], predicted Label=[0.475810984079,0.253077478581,0.27111153734]\n",
      "(Iris-setosa, Iris-setosa --> prob=[4.5,2.3,1.3,0.3], predicted Label=[0.39021221799,0.28655254839,0.32323523362]\n",
      "(Iris-setosa, Iris-setosa --> prob=[4.6,3.6,1.0,0.2], predicted Label=[0.519162280174,0.232177765704,0.248659954123]\n",
      "(Iris-setosa, Iris-setosa --> prob=[4.7,3.2,1.3,0.2], predicted Label=[0.47405000579,0.252519744855,0.273430249355]\n",
      "(Iris-setosa, Iris-setosa --> prob=[4.9,3.1,1.5,0.1], predicted Label=[0.468421196729,0.257009439772,0.274569363499]\n",
      "Coefficients:  [[ 0.          0.34253428 -0.19136101 -0.39962172]\n",
      " [ 0.          0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.13842965]] \n",
      "Intercept:  [0.140052600264,0.0116661148,-0.151718715064] \n",
      "numClasses:  3 \n",
      "numFeatures:  4\n",
      "('Test Error = ', 0.5128205128205128)\n"
     ]
    }
   ],
   "source": [
    "tmTrainingDf,tmTestDf = rawDf.randomSplit([0.7,0.3])\n",
    "labelInder = StringIndexer(inputCol='label',outputCol='indexedLabel').fit(rawDf)\n",
    "featureIndex = VectorIndexer(inputCol='features',outputCol='indexedFeatures').fit(rawDf)\n",
    "labelConverter = IndexToString(inputCol='prediction',outputCol='predictedLabel',labels=labelInder.labels)\n",
    "threeMlrPipeline = Pipeline().setStages([labelInder,featureIndex,mlr,labelConverter])\n",
    "threeMlrPipelineModel = threeMlrPipeline.fit(tmTrainingDf)\n",
    "threeMlrPredictions = threeMLrPipelineModel.transform(tmTestDf)\n",
    "result3 = threeMlrPredictions.select(\"predictedLabel\", \"label\", \"features\", \"probability\").rdd\\\n",
    ".map(lambda x:(\"(%s, %s --> prob=%s, predicted Label=%s\") %(x.predictedLabel,x.label,x.features,x.probability)).take(5)\n",
    "for i in result3:\n",
    "    print i\n",
    "tmEvaluator = MulticlassClassificationEvaluator(labelCol='indexedLabel',predictionCol='prediction')\n",
    "tmlrAccuracy = tmEvaluator.evaluate(threeMlrPredictions)\n",
    "mlrModel = threeMlrPipelineModel.stages[2]\n",
    "print \"Coefficients: \" , mlrModel.coefficientMatrix.toArray(),\"\\n\"\\\n",
    "        \"Intercept: \",mlrModel.interceptVector,\"\\n\"\\\n",
    "        \"numClasses: \",mlrModel.numClasses,\"\\n\"\\\n",
    "        \"numFeatures: \",mlrModel.numFeatures\n",
    "tmlrAccuracy = tmEvaluator.evaluate(threeMlrPredictions)\n",
    "print ('Test Error = ',1-tmlrAccuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12+"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
